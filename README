Before running the Terraform commands ( init, plan, etc... ), make sure you are logged into AWS on your terminal window.  Make sure your terminal session has access to your AWS credentials. You can do this by:

Option 1: Using environment variables

export AWS_ACCESS_KEY_ID=your-access-key-id
export AWS_SECRET_ACCESS_KEY=your-secret-access-key
export AWS_DEFAULT_REGION=us-east-2  # or your preferred region
Option 2: Using the AWS CLI (recommended)

aws configure

PS C:\data\EclipseAWSLambda\reports-demo> aws sts get-caller-identity
{
    "UserId": "AIDA4B7WW22C6EIPQDXZO",
    "Account": "828909213317",
    "Arn": "arn:aws:iam::828909213317:user/monica"
}


Helpful Terraform Links:
- [Terraform Language Documentation](https://www.terraform.io/docs/language/index.html)
- [Resource: azure_registry](https://registry.terraform.io/namespaces/Azure)

## Step 0: Initialize Terraform
```
terraform init
```

## Step 1: Plan Resources
```
terraform plan -var-file="vars/us-east-2.tfvars"
```
To save the output of terraform plan enter:

terraform plan -var-file="vars/us-east-2.tfvars" > plan-output.txt


## Step 2: Apply Resources
```
terraform apply -var-file="vars/us-east-2.tfvars"
```
To save the output of terraform apply enter:

terraform apply -var-file="vars/us-east-2.tfvars" -auto-approve > apply-output.txt


## Step 3: Cleanup Terraform Resources
```
terraform destroy -var-file="vars/east-us-2.tfvars"

PS C:\data\EclipseAWSLambda\reports-demo> terraform import -var-file="vars/us-east-2.tfvars" aws_s3_bucket.project_bucket s3upload-lambda-bucket
aws_s3_bucket.project_bucket: Importing from ID "s3upload-lambda-bucket"...
aws_s3_bucket.project_bucket: Import prepared!
  Prepared aws_s3_bucket for import
aws_s3_bucket.project_bucket: Refreshing state... [id=s3upload-lambda-bucket]

Import successful!

Use SSH for pushing to GitHub (recommended)
Since you already have your SSH key pair in ~/.ssh, you just need to:

1. Make sure your public key is added to GitHub:
Go to: https://github.com/settings/keys

Click "New SSH key"

Paste the contents of ~/.ssh/id_rsa.pub or ~/.ssh/id_ed25519.pub

2. Set the remote to use SSH instead of HTTPS:
If your current Git remote is HTTPS (e.g., https://github.com/username/repo.git), change it:


git remote set-url origin git@github.com:username/repo.git
(Replace username/repo.git with your actual repo path.)

3. Test your SSH connection:

ssh -T git@github.com
You should see something like:

Hi username! You've successfully authenticated, but GitHub does not provide shell access.


PS C:\data\EclipseAWSLambda\reports-demo> git init
Initialized empty Git repository in C:/data/EclipseAWSLambda/reports-demo/.git/

PS C:\data\EclipseAWSLambda\reports-demo> git add .

PS C:\data\EclipseAWSLambda\reports-demo> git commit -m "Initial commit"
[master (root-commit) 61587a7] Initial commit

PS C:\data\EclipseAWSLambda\reports-demo> git remote add origin git@github.com:monicamarshall/spark-reports.git

PS C:\data\EclipseAWSLambda\reports-demo> git push -u origin main
error: src refspec main does not match any.
error: failed to push some refs to 'git@github.com:monicamarshall/spark-reports.git'

PS C:\data\EclipseAWSLambda\reports-demo> git branch
* master

PS C:\data\EclipseAWSLambda\reports-demo> git branch -M main

PS C:\data\EclipseAWSLambda\reports-demo> git branch
* main
PS C:\data\EclipseAWSLambda\reports-demo> git push -u origin main
Counting objects: 37, done.
Delta compression using up to 8 threads.
Compressing objects: 100% (23/23), done.
Writing objects: 100% (37/37), 14.02 KiB | 0 bytes/s, done.
Total 37 (delta 1), reused 0 (delta 0)
remote: Resolving deltas: 100% (1/1), done.
To git@github.com:monicamarshall/spark-reports.git
 * [new branch]      main -> main
Branch main set up to track remote branch main from origin.

To remove autogenerated/unnecessary files in git repo:
To remove just the reduced POM:

git rm --cached dependency-reduced-pom.xml

echo "dependency-reduced-pom.xml" >> .gitignore

git commit -m "Remove auto-generated dependency-reduced-pom.xml"





PS C:\data\EclipseAWSLambda\reports-demo> mvn clean package -DskipTests 
PS C:\data\EclipseAWSLambda\reports-demo> docker build -t spark-spring-boot:1.0.0 . 
PS C:\data\EclipseAWSLambda\reports-demo> docker run -it --rm -p 8080:8080 
-e AWS_ACCESS_KEY_ID=******************* 
-e AWS_SECRET_ACCESS_KEY=***************  spark-spring-boot:1.0.0

pr.data.0.Current and population.json file are loaded as dataframes with Spark

Using the population dataframe the mean and the standard deviation 
of the annual US population across the years [2013, 2018] inclusive are created.

Using the dataframe from pr.data.0.Current, for every series_id, 
find the best year: the year with the max/largest sum of "value" 
for all quarters in that year. 

A report is generated with each series id, 
the best year for that series, 
and the summed value for that year. 

For example, the values look like this:

series_id	year	period	value
PRS30006011	1995	Q01	1
PRS30006011	1995	Q02	2
PRS30006011	1996	Q01	3
PRS30006011	1996	Q02	4
PRS30006012	2000	Q01	0
PRS30006012	2000	Q02	8
PRS30006012	2001	Q01	2
PRS30006012	2001	Q02	3
the report would generate the following table:

series_id	year	value
PRS30006011	1996	7
PRS30006012	2000	8

Using both dataframes 
a report is generated that provides the value 
for series_id = PRS30006032 and period = Q01 
and the population for that given year 
(if available in the population dataset). 
The below table shows an example of one row that might appear in the resulting table:

series_id	year	period	value	Population
PRS30006032	2018	Q01	1.9	327167439

ArgoCD load balancer http url to spart-reports application

http://a7da6eac37f0f4d7396f8057fa740062-1211314380.us-east-2.elb.amazonaws.com:8060/reports/generate

PS C:\data\EclipseAWSLambda\reports-demo> kubectl get pods -n spark-reports
NAME                                        READY   STATUS    RESTARTS   AGE
spark-reports-deployment-77f599ccdf-cn7r6   1/1     Running   0          9h

kubectl logs spark-reports-deployment-77f599ccdf-cn7r6 -n spark-reports


PS C:\data\EclipseAWSLambda\reports-demo> kubectl get svc sparkreports-service -n spark-reports
NAME                   TYPE           CLUSTER-IP       EXTERNAL-IP                                                               PORT(S)          AGE
sparkreports-service   LoadBalancer   172.20.179.153   a7da6eac37f0f4d7396f8057fa740062-1211314380.us-east-2.elb.amazonaws.com   8600:32277/TCP   9h

TEST THE SPARK-REPORTS MICROSERVICE WITH A BROWSER

http://a7da6eac37f0f4d7396f8057fa740062-1211314380.us-east-2.elb.amazonaws.com:8600/reports/generate

TEST THE SPARK-REPORTS MICROSERVICE WITH CURL

PS C:\data\EclipseAWSLambda\reports-demo> curl http://a7da6eac37f0f4d7396f8057fa740062-1211314380.us-east-2.elb.amazonaws.com:8600/reports/generate

CHECK KUBECTL LOGS:

PS C:\data\EclipseAWSLambda\reports-demo> kubectl logs spark-reports-deployment-77f599ccdf-cn7r6 -n spark-reports

IAM Roles for Service Accounts (IRSA).

Overview of Steps to Set Up IRSA for S3 Access:
Enable OIDC provider for your EKS cluster

Create an IAM policy for S3 access

Create an IAM role with a trust relationship for the service account

Create a Kubernetes service account annotated with the IAM role

Update your Deployment to use the service account

Step-by-Step Instructions

1. Enable OIDC provider for your EKS cluster
If not already done:

aws eks describe-cluster \
  --name spark-cluster \
  --query "cluster.identity.oidc.issuer" \
  --output text
Then associate the OIDC provider:

eksctl utils associate-iam-oidc-provider \
  --cluster spark-cluster \
  --approve


2. Create an IAM policy for S3 access
Example S3 policy (spark-reports-s3-policy.json):

{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::your-bucket-name",
        "arn:aws:s3:::your-bucket-name/*"
      ]
    }
  ]
}
Create the policy:


aws iam create-policy \
  --policy-name spark-reports-s3-policy \
  --policy-document file://spark-reports-s3-policy.json
3. Create an IAM role with a trust relationship
Use the OIDC issuer you retrieved earlier. Example with eksctl (recommended):

eksctl create iamserviceaccount \
  --name spark-reports-sa \
  --namespace spark-reports \
  --cluster spark-cluster \
  --attach-policy-arn arn:aws:iam::<ACCOUNT_ID>:policy/spark-reports-s3-policy \
  --approve \
  --override-existing-serviceaccounts
This command creates the IAM role and a Kubernetes service account spark-reports-sa annotated with the IAM role ARN.

4. Update your Deployment to use the service account
In your deployment.yaml, add:


spec:
  serviceAccountName: spark-reports-sa
Full example:


spec:
  containers:
    - name: sparkreports-container
      image: monicamarshall/sparkreports:2
      ...
  serviceAccountName: spark-reports-sa
Then apply the updated deployment:


kubectl apply -f deployment.yaml

Edit
kubectl get serviceaccount spark-reports-sa -n spark-reports -o yaml
You should see an annotation similar to:

eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_ID>:role/eksctl-spark-cluster-addon-iamserviceaccount-spark-reports-sa


Overview of Steps to Set Up IRSA for S3 Access:
Enable OIDC provider for your EKS cluster

Create an IAM policy for S3 access

Create an IAM role with a trust relationship for the service account

Create a Kubernetes service account annotated with the IAM role

Update your Deployment to use the service account

ðŸ›  Step-by-Step Instructions
1. Enable OIDC provider for your EKS cluster
If not already done:

bash
Copy
Edit
aws eks describe-cluster \
  --name spark-cluster \
  --query "cluster.identity.oidc.issuer" \
  --output text
Then associate the OIDC provider:

bash
Copy
Edit
eksctl utils associate-iam-oidc-provider \
  --cluster spark-cluster \
  --approve
2. Create an IAM policy for S3 access
Example S3 policy (spark-reports-s3-policy.json):

json
Copy
Edit
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:GetObject",
        "s3:PutObject",
        "s3:ListBucket"
      ],
      "Resource": [
        "arn:aws:s3:::your-bucket-name",
        "arn:aws:s3:::your-bucket-name/*"
      ]
    }
  ]
}
Create the policy:

bash
Copy
Edit
aws iam create-policy \
  --policy-name spark-reports-s3-policy \
  --policy-document file://spark-reports-s3-policy.json
3. Create an IAM role with a trust relationship
Use the OIDC issuer you retrieved earlier. Example with eksctl (recommended):

bash
Copy
Edit
eksctl create iamserviceaccount \
  --name spark-reports-sa \
  --namespace spark-reports \
  --cluster spark-cluster \
  --attach-policy-arn arn:aws:iam::<ACCOUNT_ID>:policy/spark-reports-s3-policy \
  --approve \
  --override-existing-serviceaccounts
This command creates the IAM role and a Kubernetes service account spark-reports-sa annotated with the IAM role ARN.

4. Update your Deployment to use the service account
In your deployment.yaml, add:

spec:
  serviceAccountName: spark-reports-sa
Full example:


spec:
  containers:
    - name: sparkreports-container
      image: monicamarshall/sparkreports:2
      ...
  serviceAccountName: spark-reports-sa
Then apply the updated deployment:


kubectl apply -f deployment.yaml
Final Check
To verify:


kubectl get serviceaccount spark-reports-sa -n spark-reports -o yaml
You should see an annotation similar to:


eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_ID>:role/eksctl-spark-cluster-addon-iamserviceaccount-spark-reports-sa


Creating a folder like eks-service-account at the top level of your project to hold the Terraform configuration for your IAM Role and Service Account (for IRSA â€“ IAM Roles for Service Accounts)


reports-demo/
â”œâ”€â”€ terraform-eks/
â”œâ”€â”€ terraform-argocd/
â”œâ”€â”€ eks-service-account/
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ variables.tf
â”‚   â””â”€â”€ outputs.tf

main.tf (in eks-service-account/)

provider "aws" {
  region = var.region
}

resource "aws_iam_role" "spark_reports_irsa_role" {
  name = "spark-reports-irsa-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17",
    Statement = [{
      Effect = "Allow",
      Principal = {
        Federated = "arn:aws:iam::${var.aws_account_id}:oidc-provider/${var.oidc_provider_url}"
      },
      Action = "sts:AssumeRoleWithWebIdentity",
      Condition = {
        StringEquals = {
          "${var.oidc_provider_url}:sub" = "system:serviceaccount:${var.namespace}:${var.service_account_name}"
        }
      }
    }]
  })
}

resource "aws_iam_role_policy_attachment" "s3_access" {
  role       = aws_iam_role.spark_reports_irsa_role.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonS3FullAccess" # Or use a custom least-privilege policy
}

resource "kubernetes_service_account" "spark_reports_sa" {
  metadata {
    name      = var.service_account_name
    namespace = var.namespace
    annotations = {
      "eks.amazonaws.com/role-arn" = aws_iam_role.spark_reports_irsa_role.arn
    }
  }
}

variables.tf

variable "region" {}
variable "aws_account_id" {}
variable "oidc_provider_url" {} # e.g., oidc.eks.us-east-2.amazonaws.com/id/EXAMPLED539D4633E53DE1B716D3041E
variable "namespace" {}
variable "service_account_name" {}
outputs.tf

output "service_account_name" {
  value = kubernetes_service_account.spark_reports_sa.metadata[0].name
}

output "iam_role_arn" {
  value = aws_iam_role.spark_reports_irsa_role.arn
}

Get OIDC Provider URL:

aws eks describe-cluster --name spark-cluster --region us-east-2 \
  --query "cluster.identity.oidc.issuer" --output text
Remove the https:// prefix when using it in var.oidc_provider_url.

Set variables (via .tfvars or directly in Terraform Cloud/CLI):

Example:


region               = "us-east-2"
aws_account_id       = "123456789012"
oidc_provider_url    = "oidc.eks.us-east-2.amazonaws.com/id/EXAMPLE"
namespace            = "spark-reports"
service_account_name = "spark-reports-sa"

terraform init

terraform apply -var-file="your-vars.tfvars"

Use the service account in your K8s deployment:

In your deployment.yaml:

spec:
  serviceAccountName: spark-reports-sa


Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run "terraform apply" now.
PS C:\data\EclipseAWSLambda\reports-demo\eks-service-account> terraform apply
var.aws_account_id
  Enter a value: 828909213317

var.namespace
  Enter a value: spark-reports

var.oidc_provider_url
  Enter a value: https://oidc.eks.us-east-2.amazonaws.com/id/59B033A89C09FC3AA6AAED928D2FC7AA

var.region
  Enter a value: us-east-2

var.service_account_name
  Enter a value: spark-reports-sa


$env:KUBECONFIG = "C:\Users\monica\.kube\config"

kubectl get deployment spark-reports-deployment -n spark-reports -o jsonpath='{.spec.template.spec.containers[0].image}'